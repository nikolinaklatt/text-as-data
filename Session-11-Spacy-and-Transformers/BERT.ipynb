{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation instructions\n",
    "\n",
    "[![colab badge](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mcallaghan/text-as-data/blob/master/Session-11-Spacy-and-Transformers/BERT.ipynb)\n",
    "\n",
    "To install the required libraries, you will need to do the following in a terminal shell (or prepend a ! to each line and run in colab)\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "pip install datasets\n",
    "pip install torch\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from torch import tensor\n",
    "from torch.nn import Sigmoid, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9998040795326233},\n",
       " {'label': 'POSITIVE', 'score': 0.999700665473938}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "pipe([\"This movie was really bad\", \"I loved watching this movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Employee_Engagement_Inclusion_And_Diversity',\n",
       "  'score': 0.9726636409759521}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"nbroad/ESG-BERT\")\n",
    "texts = [\n",
    "    \"The Hertie School is committed to embedding and mainstreaming diversity, equity and inclusion into all areas of its activities.\"\n",
    "]\n",
    "pipe(\"The Hertie School is committed to embedding and mainstreaming diversity, equity and inclusion into all areas of its activities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/max/software/py39/lib/python3.9/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Large language models can be useful. However, for non English speakers',\n",
       " 'or if not for someone who has experienced an English accent, they can',\n",
       " 'be ignored. Also, in some languages all the words have an accent, and',\n",
       " 'English language does not. For example']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import wrap\n",
    "run_galactica = False\n",
    "if run_galactica:\n",
    "    pipe = pipeline(\"text-generation\", model=\"facebook/galactica-1.3b\")\n",
    "else:\n",
    "    pipe = pipeline(\"text-generation\")\n",
    "    \n",
    "res = pipe(\"Large language models can be useful. However,\")\n",
    "wrap(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "run_galactica = False\n",
    "if run_galactica:\n",
    "    #pipe = pipeline(\"text-generation\", model=\"facebook/galactica-1.3b\")\n",
    "    #res = pipe(\"What are the benefits of taking the Text as Data course at Hertie\", max_length=200)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\")\n",
    "    model = OPTForCausalLM.from_pretrained(\"facebook/galactica-1.3b\", device_map=\"auto\")\n",
    "\n",
    "    input_text = \"Wiki page about the Text as Data Course at the Hertie School of Governance. \\n#Introduction\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids, max_length=200)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4639686048030853,\n",
       "  'token': 339,\n",
       "  'token_str': ' win',\n",
       "  'sequence': 'The GOP is going to win this election'},\n",
       " {'score': 0.4603649973869324,\n",
       "  'token': 2217,\n",
       "  'token_str': ' lose',\n",
       "  'sequence': 'The GOP is going to lose this election'},\n",
       " {'score': 0.011407200247049332,\n",
       "  'token': 8052,\n",
       "  'token_str': ' steal',\n",
       "  'sequence': 'The GOP is going to steal this election'},\n",
       " {'score': 0.008465026505291462,\n",
       "  'token': 11781,\n",
       "  'token_str': ' dominate',\n",
       "  'sequence': 'The GOP is going to dominate this election'},\n",
       " {'score': 0.00669073686003685,\n",
       "  'token': 9808,\n",
       "  'token_str': ' sweep',\n",
       "  'sequence': 'The GOP is going to sweep this election'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "unmasker(f\"The GOP is going to {unmasker.tokenizer.mask_token} this election\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take our texts and our labels again\n",
    "texts, y = zip(\n",
    "    *[\n",
    "        (\"Climate change is impacting human systems\", 1),\n",
    "        (\"Climate change is caused by fossil fuels\", 0),\n",
    "        (\"Agricultural yields are affected by climate change\", 1),\n",
    "        (\"System change not climate change\", 0),\n",
    "        (\"higher temperatures are impacting human health\", 1),\n",
    "        (\"Forest fires are becoming more frequent due to climate change\", 1),\n",
    "        (\"Machine learning can read texts\", 0),\n",
    "        (\"AI can help solve climate change!\", 0),\n",
    "        (\"We need to save gas this winter\", 0),\n",
    "        (\"More frequent droughts are impacting crop yields\", 1),\n",
    "        (\"Many communities are affected by rising sea levels\", 1),\n",
    "        (\"Global emissions continue to rise\", 0),\n",
    "        (\"Ecosystems are increasingly impacted by rising temperatures\", 1),\n",
    "        (\"Emissions from fossil fuels need to decline\", 0),\n",
    "        (\"Anthropogenic climate change is impacting vulnerable communities\", 1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b06f70e296c4550a70ed2cd6e4d59c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 15\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# To use these with transformers, we are going to need to get them into the right format.\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# First we'll put them into a HuggingFace Dataset object\n",
    "dataset = Dataset.from_dict({\"text\": texts, \"label\": y})\n",
    "\n",
    "# And now we need to tokenize the texts, using the pretrained tokenizer from climatebert\n",
    "model_name = \"climatebert/distilroberta-base-climate-f\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"longest\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can wrap this into one function that turns any set of texts (and optional labels)\n",
    "# into a tokenized huggingface dataset\n",
    "def datasetify(x, tokenizer, y=None):\n",
    "    data_dict = {\"text\": x}\n",
    "    if y is not None:\n",
    "        data_dict[\"label\"] = y\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"longest\", truncation=True)\n",
    "\n",
    "    return dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfbc770897e4f4f9d3a4f7e9ed9d927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/max/software/py39/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.5861020882924398, metrics={'train_runtime': 5.9745, 'train_samples_per_second': 7.532, 'train_steps_per_second': 1.004, 'total_flos': 300624936300.0, 'train_loss': 0.5861020882924398, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to load our model, and instantiate a Trainer class\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# We set num_labels to 2 for binary classification, as we have two classes - positive and negative\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# The trainer class needs to be supplied with a model, and a dataset (and will also accept TrainingArguments and validation data)\n",
    "trainer = Trainer(model=model, train_dataset=datasetify(texts, tokenizer, y))\n",
    "# Once this has been instantiated we can apply the train() method\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab21206046841cd8385d76efe3534f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.44497296,  0.19230443],\n",
       "       [ 0.27660358, -0.2175957 ],\n",
       "       [ 0.18573667, -0.126447  ]], dtype=float32), label_ids=array([1, 0, 0]), metrics={'test_loss': 0.4832989275455475, 'test_runtime': 0.0582, 'test_samples_per_second': 51.578, 'test_steps_per_second': 17.193})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To generate predictions, we just need to supply a dataset to the predict method\n",
    "new_texts = [\n",
    "    \"climate change is impacting terrestrial ecosystems\",\n",
    "    \"Machine Learning will solve climate change\",\n",
    "    \"Fossil fuels are responsible for rising temperature\",\n",
    "]\n",
    "\n",
    "\n",
    "pred = trainer.predict(datasetify(new_texts, tokenizer, [1, 0, 0]))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7955/30200511.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  activation(tensor(pred.predictions))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3459, 0.6541],\n",
       "        [0.6211, 0.3789],\n",
       "        [0.5774, 0.4226]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, the model output gives us logits. If these are negative, then the prediction\n",
    "# is negative, if they are positive, the prediction is positive.\n",
    "# We can turn these into probabilities with an activation function\n",
    "from torch import tensor\n",
    "from torch.nn import Sigmoid, Softmax\n",
    "\n",
    "activation = (\n",
    "    Softmax()\n",
    ")  # Since we have two *exclusive classes*, we use the Softmax function\n",
    "activation(tensor(pred.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3906, 0.5479],\n",
       "        [0.5687, 0.4458],\n",
       "        [0.5463, 0.4684]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = (\n",
    "    Sigmoid()\n",
    ")  # With a Sigmoid function, the probabilities don't need to add up to 1 (useful for multilabel classification)\n",
    "activation(tensor(pred.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a8f5284c074835bb54019e427bdb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 15\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664521ac9ba04199a68fee0a8a81ddfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7955/898640710.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return activation(tensor(logits)).numpy()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.07852197, 0.92147803],\n",
       "       [0.709726  , 0.29027405],\n",
       "       [0.5007211 , 0.4992788 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to always get probabilities, we can subclass Trainer and add a new predict_proba method\n",
    "\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "\n",
    "class ProbTrainer(Trainer):\n",
    "    def predict_proba(self, test_dataset: Dataset) -> PredictionOutput:\n",
    "        logits = self.predict(test_dataset).predictions\n",
    "        if logits.shape[1] > 2:\n",
    "            activation = Sigmoid()\n",
    "        else:\n",
    "            activation = Softmax()\n",
    "        return activation(tensor(logits)).numpy()\n",
    "\n",
    "\n",
    "trainer = ProbTrainer(model=model, train_dataset=datasetify(texts, tokenizer, y))\n",
    "trainer.train()\n",
    "\n",
    "pred = trainer.predict_proba(datasetify(new_texts, tokenizer))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "  \"batch_size\": [16, 32],\n",
    "  \"learning_rate\": [5e-5, 3e-5, 2e-5],\n",
    "  \"number of epochs\": [2,3,4]\n",
    "}\n",
    "import itertools\n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in itertools.product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "param_space = list(product_dict(**params))\n",
    "len(param_space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb96f0244fb94ecfa8d20541ceb681e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 15\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "for p in param_space:\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=p[\"number of epochs\"],\n",
    "        learning_rate=p[\"learning_rate\"],\n",
    "        per_device_train_batch_size=p[\"batch_size\"],\n",
    "        output_dir=\"out\"\n",
    "    )\n",
    "    trainer = ProbTrainer(model=model, train_dataset=datasetify(texts, tokenizer, y), args=training_args)\n",
    "    trainer.train()\n",
    "    break\n",
    "                           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
